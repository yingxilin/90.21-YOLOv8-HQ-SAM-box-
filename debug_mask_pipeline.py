#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HQ-SAM + YOLO + BoxRefinement è°ƒè¯•è„šæœ¬
æ£€æŸ¥æ©ç ç”Ÿæˆå¼‚å¸¸åŸå› ï¼ˆå…¨é»‘æ©ç è¯Šæ–­ï¼‰
"""

import os, sys, cv2, torch, numpy as np
from pathlib import Path
from ultralytics import YOLO

# ========= é¡¹ç›®è·¯å¾„ =========
SAM_HQ_PATH = r"D:\search\fungi\26\sam-hq"
BOX_REFINER_MODULE = r"D:\search\fungi\26_2\26\modules"
YOLO_WEIGHTS = r"D:\search\fungi\26\FungiTastic\runs\detect\fungi_detection\weights\best.pt"
SAM_CKPT = r"D:\search\fungi\26\data\models\fungitastic_ckpts\sam_hq_vit_h.pth"
BOX_REFINER_CKPT = r"D:\search\fungi\26_2\26\checkpoints\box_refinement\best_model.pth"
TEST_IMAGE = r"D:\search\fungi\26\data\FungiTastic-Mini\val\300p\0-3424495356.JPG"  # æ”¹æˆä»»æ„ä¸€å¼ å¼‚å¸¸å›¾
DEVICE = "cuda"
# ===========================

sys.path.append(SAM_HQ_PATH)
sys.path.append(BOX_REFINER_MODULE)
from segment_anything import sam_model_registry
from box_refinement import BoxRefinementModule


def check_tensor_info(name, t):
    """æ‰“å°å¼ é‡ç»Ÿè®¡ä¿¡æ¯"""
    if isinstance(t, torch.Tensor):
        t = t.detach().cpu().numpy()
    print(f"ğŸ“Š {name}: shape={t.shape}, mean={t.mean():.5f}, std={t.std():.5f}, min={t.min():.5f}, max={t.max():.5f}")


def main():
    device = torch.device(DEVICE if torch.cuda.is_available() else "cpu")

    # 1ï¸âƒ£ åŠ è½½ YOLO
    print("ğŸ”¹ Loading YOLO...")
    yolo = YOLO(YOLO_WEIGHTS).to(device).eval()

    # 2ï¸âƒ£ åŠ è½½ HQ-SAM
    print("ğŸ”¹ Loading HQ-SAM...")
    sam = sam_model_registry["vit_h"](checkpoint=SAM_CKPT).to(device).eval()

    # 3ï¸âƒ£ åŠ è½½ BoxRefinement
    print("ğŸ”¹ Loading Box Refinement...")
    box_refiner = BoxRefinementModule(hidden_dim=256, num_heads=8, max_offset=50).to(device)
    ckpt = torch.load(BOX_REFINER_CKPT, map_location=device)
    box_refiner.load_state_dict(ckpt, strict=False)
    box_refiner.eval()

    # 4ï¸âƒ£ è¯»å–æµ‹è¯•å›¾åƒ
    print(f"\nğŸ–¼ï¸ Testing image: {TEST_IMAGE}")
    img = cv2.imread(TEST_IMAGE)
    if img is None:
        print("âŒ Image not found!")
        return
    H, W = img.shape[:2]
    print(f"Image shape: {H}x{W}")

    # 5ï¸âƒ£ YOLO æ£€æµ‹
    with torch.no_grad():
        results = yolo.predict(source=str(TEST_IMAGE), conf=0.35, verbose=False)[0]
    boxes = results.boxes.xyxy.cpu().numpy() if len(results.boxes) > 0 else np.array([])
    print(f"YOLO detections: {len(boxes)} boxes")
    if len(boxes) > 0:
        print(f"First box: {boxes[0]}")

    # 6ï¸âƒ£ HQ-SAM ç¼–ç 
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_tensor = torch.as_tensor(img_rgb, device=device).permute(2, 0, 1).float() / 255.0
    img_1024 = torch.nn.functional.interpolate(
        img_tensor.unsqueeze(0), size=(1024, 1024), mode="bilinear", align_corners=False
    )
    with torch.no_grad():
        encoder_out = sam.image_encoder(img_1024)
    image_embedding = encoder_out[0] if isinstance(encoder_out, (tuple, list)) else encoder_out
    interm_emb = encoder_out[1] if isinstance(encoder_out, (tuple, list)) and len(encoder_out) > 1 else None
    check_tensor_info("SAM image_embedding", image_embedding)

        # 7ï¸âƒ£ Box Refinement
    if len(boxes) == 0:
        print("âš ï¸ No YOLO boxes found. Skipping refinement.")
        return

    box = boxes[0]
    bbox_norm = torch.tensor([[box[0] / W, box[1] / H, box[2] / W, box[3] / H]], device=device).float()

    # ğŸ”§ ç¡®ä¿ image_embedding æ˜¯ float32
    if image_embedding.dtype != torch.float32:
        image_embedding = image_embedding.float()

    with torch.no_grad():
        refined_bbox, _ = box_refiner.iterative_refine(image_embedding, bbox_norm, (H, W), max_iter=3)

    refined_bbox = refined_bbox.squeeze(0).cpu().numpy() * np.array([W, H, W, H])
    print(f"Refined box: {refined_bbox}")


    # 8ï¸âƒ£ HQ-SAM mask decoder
    boxes_1024 = refined_bbox.copy()
    boxes_1024[[0, 2]] *= (1024.0 / W)
    boxes_1024[[1, 3]] *= (1024.0 / H)
    boxes_1024 = torch.tensor([boxes_1024], device=device, dtype=torch.float32)

    with torch.no_grad():
        sparse_emb, dense_emb = sam.prompt_encoder(points=None, boxes=boxes_1024, masks=None)
        mask_logits, _ = sam.mask_decoder(
            image_embeddings=image_embedding,
            image_pe=sam.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_emb,
            dense_prompt_embeddings=dense_emb,
            multimask_output=False,
            hq_token_only=True,
            interm_embeddings=interm_emb,
        )

    check_tensor_info("mask_logits", mask_logits)

    mask = torch.sigmoid(mask_logits).cpu().numpy()[0, 0]
    print(f"Mask mean intensity: {mask.mean():.5f}, min={mask.min():.5f}, max={mask.max():.5f}")
    mask_resized = cv2.resize(mask, (W, H))
    mask_bin = (mask_resized > 0.5).astype(np.uint8) * 255
    nonzero_ratio = mask_bin.sum() / (H * W * 255)
    print(f"Nonzero pixel ratio: {nonzero_ratio*100:.2f}%")

    cv2.imwrite(str(Path(BOX_REFINER_MODULE) / "debug_mask_output.png"), mask_bin)
    print(f"\nâœ… Debug mask saved to: {Path(BOX_REFINER_MODULE) / 'debug_mask_output.png'}")

    # 9ï¸âƒ£ æœ€ç»ˆåˆ¤æ–­
    print("\n==== DIAGNOSIS ====")
    if len(boxes) == 0:
        print("âŒ YOLO æ£€æµ‹æ— ç»“æœ â†’ æ£€æŸ¥æ£€æµ‹é˜ˆå€¼")
    elif image_embedding.abs().mean() < 1e-5:
        print("âŒ SAM image_embedding å…¨é›¶ â†’ æ£€æŸ¥ SAM æ¨¡å‹åŠ è½½æˆ–å›¾åƒé¢„å¤„ç†")
    elif np.any(refined_bbox < 0) or np.any(refined_bbox > max(H, W)):
        print("âš ï¸ BoxRefinement è¾“å‡ºè¶Šç•Œ â†’ æ£€æŸ¥ iterative_refine() çš„å½’ä¸€åŒ–é€»è¾‘")
    elif mask.mean() < 0.001:
        print("âŒ mask_decoder è¾“å‡ºå…¨é›¶ â†’ HQ-SAM æ²¡æœ‰æ­£ç¡®å“åº”æç¤º")
    elif nonzero_ratio < 0.01:
        print("âš ï¸ æ©ç å‡ ä¹å…¨é»‘ â†’ å¯èƒ½æç¤ºæ¡†å¤ªå°æˆ–æ— æ•ˆ")
    else:
        print("âœ… ä¸€åˆ‡æ­£å¸¸ã€‚SAM è¾“å‡ºéç©ºï¼Œæ©ç åº”å¯ç”¨ã€‚")


if __name__ == "__main__":
    main()
