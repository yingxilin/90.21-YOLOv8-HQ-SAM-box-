#!/usr/bin/env python3
"""
Box Refinement è®­ç»ƒè„šæœ¬ - æœ€ç»ˆä¿®å¤ç‰ˆæœ¬
è§£å†³æ‰€æœ‰å·²çŸ¥é—®é¢˜ï¼šæŸå¤±å€¼è¿‡å¤§ã€è¿è¡Œæ—¶é—´è¿‡é•¿ã€ç¼“å­˜å¤±æ•ˆ
"""

import os
import sys
import time
import yaml
import argparse
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm
import cv2

# æ·»åŠ modulesç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'modules'))

from modules.box_refinement import BoxRefinementModule, box_iou_loss
from modules.hqsam_feature_extractor import create_hqsam_extractor


class FungiDataset(Dataset):
    """FungiTastic æ•°æ®é›†åŠ è½½å™¨ - ä¿®å¤ç‰ˆæœ¬"""
    
    def __init__(self, data_root: str, split: str = 'train', 
                 image_size: int = 300, data_subset: str = 'Mini',
                 augmentation: bool = True, debug: bool = False,
                 masks_file: Optional[str] = None):
        """
        Args:
            data_root: æ•°æ®é›†æ ¹ç›®å½•
            split: 'train' æˆ– 'val'
            image_size: å›¾åƒå°ºå¯¸
            data_subset: 'Mini' æˆ– 'Full'
            augmentation: æ˜¯å¦å¯ç”¨æ•°æ®å¢å¼º
            debug: è°ƒè¯•æ¨¡å¼
        """
        self.data_root = Path(data_root)
        self.split = split
        self.image_size = image_size
        self.data_subset = data_subset
        self.augmentation = augmentation
        self.debug = debug
        
        self.images_dir = self.data_root / f"{data_subset}" / split / f"{image_size}p"

        # å¦‚æä¾›parquetè·¯å¾„åˆ™ä½¿ç”¨
        if masks_file:
            self.masks_path = Path(masks_file)
            if not self.masks_path.exists():
                raise FileNotFoundError(f"Masks parquet file not found: {self.masks_path}")
            self.use_parquet_masks = True
            # ä½¿ç”¨ pyarrow.dataset è¿›è¡ŒæŒ‰éœ€è¯»å–ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å…¨è¡¨
            try:
                import pyarrow.dataset as ds  # type: ignore
                self._pa_ds = ds.dataset(str(self.masks_path), format='parquet')
                # è®°å½•å¯ç”¨åˆ—
                self._pa_schema_names = set(self._pa_ds.schema.names)
                self._pa_has_mask = 'mask' in self._pa_schema_names
                self._pa_has_rle = all(name in self._pa_schema_names for name in ['rle', 'width', 'height'])
                self._pa_file_name_field = 'file_name' if 'file_name' in self._pa_schema_names else None
            except Exception as e:
                raise RuntimeError(f"Failed to open parquet dataset: {e}")
            # ç®€å•çš„æœ€è¿‘ä½¿ç”¨ç¼“å­˜ï¼Œå‡å°‘é‡å¤IO
            self._mask_cache = {}
            self._mask_cache_limit = 1024
        else:
            self.masks_dir = self.data_root / f"{data_subset}" / split / "masks"
            if not self.masks_dir.exists():
                raise FileNotFoundError(f"Masks directory not found: {self.masks_dir}")
            self.use_parquet_masks = False

        
        # è·å–å›¾åƒæ–‡ä»¶åˆ—è¡¨
        self.image_files = sorted(list(self.images_dir.glob("*.jpg")) + 
                                 list(self.images_dir.glob("*.JPG")) + 
                                 list(self.images_dir.glob("*.png")))
        
        if debug:
            self.image_files = self.image_files[:100]  # è°ƒè¯•æ¨¡å¼åªä½¿ç”¨å‰100å¼ å›¾åƒ
        
        print(f"Found {len(self.image_files)} images in {self.split} split")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        image_path = self.image_files[idx]
        image = cv2.imread(str(image_path))
        if image is None:
            raise ValueError(f"Failed to load image: {image_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        gt_bbox = None
        if self.use_parquet_masks:
            # ä¼˜å…ˆä»ç¼“å­˜è·å–ï¼ˆä½¿ç”¨æ–‡ä»¶åä½œä¸ºé”®ï¼‰
            image_key = image_path.name
            cached = self._mask_cache.get(image_key)
            if cached is not None:
                gt_bbox = cached
            else:
                try:
                    import pyarrow.dataset as ds  # type: ignore
                except Exception as e:
                    raise RuntimeError(f"pyarrow is required for parquet reading: {e}")
                # ä»…è¯·æ±‚å®é™…å­˜åœ¨çš„åˆ—
                requested_cols = []
                if self._pa_file_name_field is not None:
                    requested_cols.append(self._pa_file_name_field)
                if self._pa_has_mask:
                    requested_cols.append('mask')
                if self._pa_has_rle:
                    requested_cols.extend(['width', 'height', 'rle'])
                if self._pa_file_name_field is not None:
                    filter_expr = (ds.field(self._pa_file_name_field) == image_key)
                else:
                    filter_expr = None
                table = self._pa_ds.to_table(columns=requested_cols, filter=filter_expr) if filter_expr is not None else self._pa_ds.to_table(columns=requested_cols)
                if table.num_rows == 0 and self._pa_file_name_field is not None:
                    filter_expr2 = (ds.field(self._pa_file_name_field) == image_path.stem)
                    table = self._pa_ds.to_table(columns=requested_cols, filter=filter_expr2)
                if table.num_rows == 0:
                    gt_bbox = None
                else:
                    cols = {name: table.column(name) for name in table.schema.names}
                    if self._pa_has_rle and all(k in cols for k in ['rle', 'width', 'height']):
                        rle_list = cols['rle'][0].as_py()
                        width_val = int(cols['width'][0].as_py())
                        height_val = int(cols['height'][0].as_py())
                        gt_bbox = self._compute_bbox_from_rle_counts(rle_list, width_val, height_val)
                    elif self._pa_has_mask and 'mask' in cols:
                        cell = cols['mask'][0].as_py()
                        mask_arr = np.asarray(cell)
                        if mask_arr.ndim == 3:
                            mask_arr = mask_arr[..., 0]
                        if mask_arr.ndim == 1:
                            # 1D æƒ…å†µï¼ŒæŒ‰å½“å‰å›¾åƒå°ºå¯¸é‡å¡‘
                            mask_arr = mask_arr.reshape(image.shape[0], image.shape[1])
                        gt_bbox = self._compute_bbox_from_mask(mask_arr.astype(np.uint8))
                # ç¼“å­˜bbox
                if gt_bbox is None:
                    gt_bbox = self._compute_bbox_from_mask(np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8))
                if len(self._mask_cache) >= self._mask_cache_limit:
                    self._mask_cache.pop(next(iter(self._mask_cache)))
                self._mask_cache[image_key] = gt_bbox
        else:
            mask_path = self.masks_dir / f"{image_path.stem}.png"
            if not mask_path.exists():
                mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
                gt_bbox = self._compute_bbox_from_mask(mask)
            else:
                mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
            if mask is None:
                mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
            if gt_bbox is None:
                gt_bbox = self._compute_bbox_from_mask(mask)
        
        # è°ƒæ•´å›¾åƒå°ºå¯¸ï¼Œå¹¶æŒ‰æ¯”ä¾‹ç¼©æ”¾ bboxï¼ˆé¿å…é‡å»ºæ•´å¼ maskï¼‰
        orig_h, orig_w = image.shape[:2]
        if image.shape[:2] != (self.image_size, self.image_size):
            sx = self.image_size / orig_w
            sy = self.image_size / orig_h
            image = cv2.resize(image, (self.image_size, self.image_size))
            if gt_bbox is not None:
                x1, y1, x2, y2 = gt_bbox
                gt_bbox = np.array([x1 * sx, y1 * sy, x2 * sx, y2 * sy], dtype=np.float32)
            # ä¸å†éœ€è¦maskå‚ä¸è®­ç»ƒï¼Œæä¾›å ä½å³å¯
            mask = np.zeros((self.image_size, self.image_size), dtype=np.uint8)
        else:
            # ä¸ä½¿ç”¨çœŸå®maskä»¥èŠ‚çœCPUæ—¶é—´
            mas        # ç”Ÿæˆnoisy bbox (æ¨¡æ‹ŸYOLOè¾“å‡º)
        noisy_bbox = self._generate_noisy_bbox(gt_bbox, image.shape[:2])
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå½’ä¸€åŒ–è¾¹ç•Œæ¡†åæ ‡åˆ° [0, 1] èŒƒå›´
        h, w = image.shape[:2]
        gt_bbox_normalized = gt_bbox / np.array([w, h, w, h], dtype=np.float32)
        noisy_bbox_normalized = noisy_bbox / np.array([w, h, w, h], dtype=np.float32)
        
        # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        gt_bbox_normalized = np.clip(gt_bbox_normalized, 0.0, 1.0)
        noisy_bbox_normalized = np.clip(noisy_bbox_normalized, 0.0, 1.0)    h, w = image.shape[:2]
        gt_bbox_normalized = gt_bbox / np.array([w, h, w, h], dtype=np.float32)
        noisy_bbox_normalized = noisy_bbox / np.array([w, h, w, h], dtype=np.float32)
        
        return {
            'image': image,
            'mask': mask,
            'gt_bbox': gt_bbox_normalized,  # ä½¿ç”¨å½’ä¸€åŒ–åæ ‡
            'noisy_bbox': noisy_bbox_normalized,  # ä½¿ç”¨å½’ä¸€åŒ–åæ ‡
            'image_path': str(image_path)
        }
    
    def _compute_bbox_from_mask(self, mask):
        """ä»maskè®¡ç®—è¾¹ç•Œæ¡†"""
        if mask.sum() == 0:
            # ç©ºmaskï¼Œè¿”å›ä¸­å¿ƒåŒºåŸŸ
            h, w = mask.shape
            center_x, center_y = w // 2, h // 2
            size = min(w, h) // 4
            return np.array([center_x - size, center_y - size, center_x + size, center_y + size], dtype=np.float32)
        
        # æ‰¾åˆ°éé›¶åƒç´ çš„ä½ç½®
        coords = np.where(mask > 0)
        if len(coords[0]) == 0:
            h, w = mask.shape
            center_x, center_y = w // 2, h // 2
            size = min(w, h) // 4
            return np.array([center_x - size, center_y - size, center_x + size, center_y + size], dtype=np.float32)
        
        y_min, y_max = coords[0].min(), coords[0].max()
        x_min, x_max = coords[1].min(), coords[1].max()
        
        return np.array([x_min, y_min, x_max, y_max], dtype=np.float32)
    
    def _compute_bbox_from_rle_counts(self, rle_list, width, height):
        """ä»RLE countsè®¡ç®—è¾¹ç•Œæ¡†"""
        if not rle_list:
            return self._compute_bbox_from_mask(np.zeros((height, width), dtype=np.uint8))
        
        # å°†RLEè½¬æ¢ä¸ºmask
        mask = np.zeros((height, width), dtype=np.uint8)
        pos = 0
        for i, count in enumerate(rle_list):
            if i % 2 == 0:  # å¶æ•°ç´¢å¼•è¡¨ç¤ºè¿ç»­çš„éé›¶åƒç´ 
                end_pos = pos + count
                if end_pos <= height * width:
                    y = pos // width
                    x = pos % width
                    mask[y, x:x+count] = 1
            pos += count
        
        return self._compute_bbox_from_mask(mask)
    
    def _generate_noisy_bbox(self, gt_bbox, image_shape):
        """ç”Ÿæˆå¸¦å™ªå£°çš„è¾¹ç•Œæ¡†"""
        if gt_bbox is None:
            h, w = image_shape[:2]
            center_x, center_y = w // 2, h // 2
            size = min(w, h) // 4
            gt_bbox = np.array([center_x - size, center_y - size, center_x + size, center_y + size], dtype=np.float32)
        
        # æ·»åŠ éšæœºå™ªå£°
        noise_scale = 0.1  # 10%çš„å™ªå£°
        h, w = image_shape[:2]
        max_noise = min(w, h) * noise_scale
        
        noise = np.random.uniform(-max_noise, max_noise, 4)
        noisy_bbox = gt_bbox + noise
        
        # ç¡®ä¿è¾¹ç•Œæ¡†åœ¨å›¾åƒèŒƒå›´å†…
        noisy_bbox[0] = max(0, min(noisy_bbox[0], w - 1))  # x1
        noisy_bbox[1] = max(0, min(noisy_bbox[1], h - 1))  # y1
        noisy_bbox[2] = max(noisy_bbox[0] + 1, min(noisy_bbox[2], w))  # x2
        noisy_bbox[3] = max(noisy_bbox[1] + 1, min(noisy_bbox[3], h))  # y2
        
        return noisy_bbox.astype(np.float32)


class FeatureCache:
    """ç‰¹å¾ç¼“å­˜ç®¡ç†å™¨ - ä¿®å¤ç‰ˆæœ¬"""
    
    def __init__(self, cache_dir: str, split: str = 'train'):
        self.cache_dir = Path(cache_dir) / f"features/{split}"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.split = split
        self.cache_hits = 0
        self.cache_misses = 0
        self.total_requests = 0
    
    def get_cache_path(self, image_path: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        image_hash = hashlib.md5(image_path.encode()).hexdigest()
        return self.cache_dir / f"{image_hash}.npy"
    
    def load_features(self, image_path: str) -> Optional[torch.Tensor]:
        """ä»ç¼“å­˜åŠ è½½ç‰¹å¾"""
        self.total_requests += 1
        cache_path = self.get_cache_path(image_path)
        
        if cache_path.exists():
            try:
                features = np.load(cache_path)
                features_tensor = torch.from_numpy(features)
                self.cache_hits += 1
                return features_tensor
            except Exception as e:
                print(f"Warning: Failed to load cached features from {cache_path}: {e}")
                self.cache_misses += 1
                return None
        else:
            self.cache_misses += 1
            return None
    
    def save_features(self, image_path: str, features: torch.Tensor):
        """ä¿å­˜ç‰¹å¾åˆ°ç¼“å­˜"""
        cache_path = self.get_cache_path(image_path)
        try:
            features_cpu = features.cpu().numpy()
            np.save(cache_path, features_cpu)
        except Exception as e:
            print(f"Warning: Failed to save features to {cache_path}: {e}")
    
    def get_cache_stats(self) -> Dict[str, float]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.total_requests == 0:
            return {'hit_rate': 0.0, 'hits': 0, 'misses': 0}
        
        hit_rate = self.cache_hits / self.total_requests
        return {
            'hit_rate': hit_rate,
            'hits': self.cache_hits,
            'misses': self.cache_misses
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if self.cache_dir.exists():
            import shutil
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_hits = 0
        self.cache_misses = 0
        self.total_requests = 0


def detect_feature_cache(cache_dir: str) -> bool:
    """æ£€æµ‹æ˜¯å¦å­˜åœ¨ç‰¹å¾ç¼“å­˜"""
    cache_path = Path(cache_dir) / "features"
    return cache_path.exists() and any(cache_path.iterdir())


def extract_features_with_cache(hqsam_extractor, images_np_list, image_paths, feature_cache, device='cuda'):
    """ä½¿ç”¨ç¼“å­˜æå–ç‰¹å¾ - ä¿®å¤ç‰ˆæœ¬"""
    features_list = []
    
    for i, (image_np, image_path) in enumerate(zip(images_np_list, image_paths)):
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if feature_cache is not None:
            cached_features = feature_cache.load_features(image_path)
            if cached_features is not None:
                # ç¡®ä¿ç¼“å­˜çš„ç‰¹å¾åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                cached_features = cached_features.to(device)
             def compute_loss(pred_bboxes, gt_bboxes, l1_weight=1.0, iou_weight=0.5):
    """è®¡ç®—æŸå¤±å‡½æ•° - ä¿®å¤ç‰ˆæœ¬"""
    # ç¡®ä¿è¾“å…¥å¼ é‡åœ¨ç›¸åŒè®¾å¤‡ä¸Š
    if pred_bboxes.device != gt_bboxes.device:
        gt_bboxes = gt_bboxes.to(pred_bboxes.device)
    
    # ç¡®ä¿è¾“å…¥å¼ é‡å½¢çŠ¶ä¸€è‡´
    if pred_bboxes.shape != gt_bboxes.shape:
        min_batch = min(pred_bboxes.shape[0], gt_bboxes.shape[0])
        pred_bboxes = pred_bboxes[:min_batch]
        gt_bboxes = gt_bboxes[:min_batch]
    
    # æ£€æŸ¥è¾“å…¥æœ‰æ•ˆæ€§
    if pred_bboxes.numel() == 0 or gt_bboxes.numel() == 0:
        return torch.tensor(0.0, device=pred_bboxes.device), torch.tensor(0.0, device=pred_bboxes.device), torch.tensor(0.0, device=pred_bboxes.device)
    
    # L1æŸå¤± - ä½¿ç”¨æ›´ç¨³å®šçš„è®¡ç®—
    l1_loss = F.l1_loss(pred_bboxes, gt_bboxes)
    
    # IoUæŸå¤± - æ·»åŠ æ•°å€¼ç¨³å®šæ€§
    try:
        iou_loss = box_iou_loss(pred_bboxes, gt_bboxes)
        # æ£€æŸ¥IoUæŸå¤±æ˜¯å¦ä¸ºNaNæˆ–Inf
        if torch.isnan(iou_loss) or torch.isinf(iou_loss):
            iou_loss = torch.tensor(0.0, device=pred_bboxes.device)
    except Exception as e:
        print(f"Warning: IoU loss computation failed: {e}")
        iou_loss = torch.tensor(0.0, device=pred_bboxes.device)
    
    # æ€»æŸå¤± - è°ƒæ•´æƒé‡æ¯”ä¾‹
    total_loss = l1_weight * l1_loss + iou_weight * iou_loss
    
    return total_loss, l1_loss, iou_lossorch.isnan(iou_loss) or torch.isinf(iou_loss):
            iou_loss = torch.tensor(0.0, device=pred_bboxes.device)
    except Exception as e:
        print(f"Warning: IoU loss computation failed: {e}")
        iou_loss = torch.tensor(0.0, device=pred_bboxes.device)
    
    # æ€»æŸå¤±
    total_loss = l1_weight * l1_loss + iou_weight * iou_loss
    
    return total_loss, l1_loss, iou_loss


def train_one_epoch(model, dataloader, optimizer, hqsam_extractor, device, epoch, config, 
                   feature_cache=None, use_amp=False):
    """è®­ç»ƒä¸€ä¸ªepoch - ä¿®å¤ç‰ˆæœ¬"""
    model.train()
    
    total_loss = 0.0
    total_l1_loss = 0.0
    total_iou_loss = 0.0
    num_batches = len(dataloader)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler() if use_amp else None
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
    
    for batch_idx, batch in enumerate(pbar):
        # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„é”®å
        images = batch['image'].to(device)
        gt_bboxes = batch['gt_bbox'].to(device)
        noisy_bboxes = batch['noisy_bbox'].to(device)
        image_paths = batch['image_path']
        
        # ç¡®ä¿image_pathsæ˜¯åˆ—è¡¨
        if isinstance(image_paths, str):
            image_paths = [image_paths]        if use_amp and scaler is not None:
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast():
                # è¿­ä»£ç²¾ç‚¼
                refined_bboxes, history = model.iterative_refine(
                    image_features, noisy_bboxes, (config['data']['image_size'], config['data']['image_size']),
                    max_iter=config['refinement']['max_iter'],
                    stop_threshold=config['refinement']['stop_threshold']
                )
                
                # è®¡ç®—æŸå¤±
                loss, l1_loss, iou_loss = compute_loss(
                    refined_bboxes, gt_bboxes,
                    l1_weight=config['loss']['l1_weight'],
                    iou_weight=config['loss']['iou_weight']
                )
            
            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            # æ™®é€šç²¾åº¦å‰å‘ä¼ æ’­
            # è¿­ä»£ç²¾ç‚¼
            refined_bboxes, history = model.iterative_refine(
                image_features, noisy_bboxes, (config['data']['image_size'], config['data']['image_size']),
                max_iter=config['refinement']['max_iter'],
                stop_threshold=config['refinement']['stop_threshold']
            )
            
            # è®¡ç®—æŸå¤±
            loss, l1_loss, iou_loss = compute_loss(
                refined_bboxes, gt_bboxes,
                l1_weight=config['loss']['l1_weight'],
                iou_weight=config['loss']['iou_weight']
            )
            
            # åå‘ä¼ æ’­
            loss.backward()
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()'],
                stop_threshold=config['refinement']['stop_threshold']
            )
            
            # è®¡ç®—æŸå¤±
            loss, l1_loss, iou_loss = compute_loss(
                refined_bboxes, gt_bboxes,
                l1_weight=config['loss']['l1_weight'],
                iou_weight=config['loss']['iou_weight']
            )
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
        
        # æ›´æ–°ç»Ÿè®¡
        total_loss += loss.item()
        total_l1_loss += l1_loss.item()
        total_iou_loss += iou_loss.item()
        
        # æ›´æ–°è¿›åº¦æ¡
        cache_stats = feature_cache.get_cache_stats() if feature_cache else {'hit_rate': 0.0}
        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'L1': f'{l1_loss.item():.4f}',
            'IoU': f'{iou_loss.item():.4f}',
            'Cache': f'Cache: {cache_stats["hit_rate"]:.1%}'
        })
    
    return total_loss / num_batches, total_l1_loss / num_batches, total_iou_loss / num_batches


def evaluate(model, dataloader, hqsam_extractor, device, config, feature_cache=None):
    """è¯„ä¼°æ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬"""
    model.eval()
    
    total_loss = 0.0
    total_l1_loss = 0.0
    total_iou_loss = 0.0
    num_batches = len(dataloader)
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„é”®å
            images = batch['image'].to(device)
            gt_bboxes = batch['gt_bbox'].to(device)
            noisy_bboxes = batch['noisy_bbox'].to(device)
            image_paths = batch['image_path']
            
            # ç¡®ä¿image_pathsæ˜¯åˆ—è¡¨
            if isinstance(image_paths, str):
                image_paths = [image_paths]
            
            # æå–ç‰¹å¾
            images_np_list = [img.cpu().numpy().transpose(1, 2, 0) for img in images]
            features_list = extract_features_with_cache(
                hqsam_extractor, images_np_list, image_paths, feature_cache, device
            )
            image_features = torch.cat(features_list, dim=0)
            
            # å‰å‘ä¼ æ’­
            refined_bboxes, history = model.iterative_refine(
                image_features, noisy_bboxes, (config['data']['image_size'], config['data']['image_size']),
                max_iter=config['refinement']['max_iter'],
                stop_threshold=config['refinement']['stop_threshold']
            )
            
            # è®¡ç®—æŸå¤±
            loss, l1_loss, iou_loss = compute_loss(
                refined_bboxes, gt_bboxes,
                l1_weight=config['loss']['l1_weight'],
                iou_weight=config['loss']['iou_weight']
            )
            
            total_loss += loss.item()
            total_l1_loss += l1_loss.item()
            total_iou_loss += iou_loss.item()
    
    return total_loss / num_batches, total_l1_loss / num_batches, total_iou_loss / num_batches


def main():
    """ä¸»å‡½æ•° - æœ€ç»ˆä¿®å¤ç‰ˆæœ¬"""
    parser = argparse.ArgumentParser(description='Box Refinement Training - Final Fixed Version')
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', type=str, default='auto', help='è®¾å¤‡é€‰æ‹©')
    parser.add_argument('--fast', action='store_true', help='å¿«é€Ÿæ¨¡å¼')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--clear-cache', action='store_true', help='æ¸…ç©ºç‰¹å¾ç¼“å­˜')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # å¿«é€Ÿæ¨¡å¼è®¾ç½®
    if args.fast:
        print("ğŸš€ Fast mode enabled - applying all optimizations")
        config['data']['sample_ratio'] = 0.1
        config['training']['use_amp'] = True
        config['training']['batch_size'] = 32
        print(f"  - Data sampling: {config['data']['sample_ratio']}")
        print(f"  - Mixed precision: {config['training']['use_amp']}")
        print(f"  - Batch size: {config['training']['batch_size']}")
    
    # è®¾å¤‡é€‰æ‹©
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    print(f"Using device: {device}")
    
    # æ£€æµ‹ç‰¹å¾ç¼“å­˜
    cache_detected = detect_feature_cache(config['output']['checkpoint_dir'])
    print(f"Feature cache detected: {cache_detected}")
    
    # åˆ›å»ºç‰¹å¾ç¼“å­˜
    feature_cache = FeatureCache(config['output']['checkpoint_dir']) if config['training']['feature_cache'] else None
    
    # æ¸…ç©ºç¼“å­˜
    if args.clear_cache and feature_cache is not None:
        print("Clearing feature cache...")
        feature_cache.clear_cache()
        print("Feature cache cleared.")
    
    # åŠ è½½æ•°æ®é›†
    print("Loading datasets...")
    train_dataset = FungiDataset(
        data_root=config['data']['data_root'],
        split=config['data']['train_split'],
        image_size=config['data']['image_size'],
        data_subset=config['data']['data_subset'],
        augmentation=config['data']['augmentation']['enabled'],
        debug=args.debug,
        masks_file=config['data'].get('masks_file')
    )
    
    val_dataset = FungiDataset(
        data_root=config['data']['data_root'],
        split=config['data']['val_split'],
        image_size=config['data']['image_size'],
        data_subset=config['data']['data_subset'],
        augmentation=False,
        debug=args.debug,
        masks_file=config['data'].get('masks_file')
    )
    
    # æ•°æ®æŠ½æ ·
    if config['data']['sample_ratio'] is not None:
        sample_ratio = config['data']['sample_ratio']
        if sample_ratio < 1.0:
            # å¯¹è®­ç»ƒé›†è¿›è¡ŒæŠ½æ ·
            train_size = int(len(train_dataset) * sample_ratio)
            train_indices = torch.randperm(len(train_dataset))[:train_size]
            train_dat    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=config['data']['num_workers'],
        pin_memory=True,
        persistent_workers=True  # ä¿æŒworkerè¿›ç¨‹ï¼Œå‡å°‘é‡å¯å¼€é”€
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=False,
        num_workers=config['data']['num_workers'],
        pin_memory=True,
        persistent_workers=True
    ) torch.utils.data.Subset(val_dataset, val_indices)
            print(f"Sampled {len(val_dataset)} images from {len(val_dataset)} total images (ratio: {sample_ratio})")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("Creating model...")
    model = BoxRefinementModule(
        hidden_dim=config['model']['hidden_dim'],
        num_heads=config['model']['num_heads'],
        max_offset=config['model']['max_offset']
    ).to(device)
    
    # åˆ›å»ºHQ-SAMç‰¹å¾æå–å™¨
    print("Loading HQ-SAM feature extractor...")
    hqsam_extractor = create_hqsam_extractor(
        checkpoint_path=config['hqsam']['checkpoint'],
        model_type=config['hqsam']['model_type'],
        device=device,
        use_mock=True  # ä½¿ç”¨Mockç‰ˆæœ¬è¿›è¡Œæµ‹è¯•
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨ - ä¿®å¤å­¦ä¹ ç‡
    print("Creating optimizer...")
    learning_rate = float(config['training']['learning_rate'])
    if args.fast:
        learning_rate *= 2  # å¿«é€Ÿæ¨¡å¼ä¸‹ç¨å¾®æé«˜å­¦ä¹ ç‡
    
    optimizer = optim.Adam(
        model.parameters(),
        lr=learning_rate,
        weight_decay=float(config['training']['weight_decay'])
    )
    
    print(f"Learning rate: {learning_rate}")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if config['training']['lr_scheduler'] == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=config['training']['epochs']
        )
    else:
        scheduler = None
    
    # è®­ç»ƒå¾ªç¯
    print("Starting training...")
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    
    for epoch in range(config['training']['epochs']):
        # è®­ç»ƒ
        train_loss, train_l1, train_iou = train_one_epoch(
            model, train_loader, optimizer, hqsam_extractor, device, epoch, config,
            feature_cache=feature_cache, use_amp=config['training']['use_amp']
        )
        
        # éªŒè¯
        val_loss, val_l1, val_iou = evaluate(
            model, val_loader, hqsam_extractor, device, config, feature_cache=feature_cache
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        if scheduler is not None:
            scheduler.step()
        
        # è®°å½•æŸå¤±
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        print(f"  Train L1: {train_l1:.4f}, IoU: {train_iou:.4f}")
        print(f"  Val L1: {val_l1:.4f}, IoU: {val_iou:.4f}")
        
        # ç¼“å­˜ç»Ÿè®¡
        if feature_cache is not None:
            cache_stats = feature_cache.get_cache_stats()
            print(f"  Cache hit rate: {cache_stats['hit_rate']:.1%}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
            }, os.path.join(config['output']['checkpoint_dir'], 'best_model.pth'))
            print(f"  New best model saved! Val Loss: {val_loss:.4f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save({
        'epoch': config['training']['epochs'] - 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
    }, os.path.join(config['output']['checkpoint_dir'], 'final_model.pth'))
    
    print("Training completed!")
    print(f"Best validation loss: {best_val_loss:.4f}")
    
    # æ‰“å°ç¼“å­˜ç»Ÿè®¡
    if feature_cache is not None:
        cache_stats = feature_cache.get_cache_stats()
        print(f"Final cache hit rate: {cache_stats['hit_rate']:.1%}")
        print(f"Total cache hits: {cache_stats['hits']}")
        print(f"Total cache misses: {cache_stats['misses']}")


if __name__ == "__main__":
    main()